#!/usr/bin/env python3
# train_model.py
# Automatically scrapes real Reddit data, labels it using heuristics, and trains the model

import joblib
import numpy as np
from sklearn.linear_model import LogisticRegression
from scipy.sparse import hstack
from features import build_feature_matrix, TextVectorizer, clickbait_score, SUBREDDIT_RISK, MISINFO_KEYWORDS
from google_scraper import search_reddit_by_hashtag

def auto_label_post(post):
    """
    Automatically label a post as misinformation (1) or normal (0) using heuristics
    """
    text = f"{post.get('title', '')} {post.get('snippet', '')}".lower()
    subreddit = post.get('subreddit', '').lower()

    # Score based on multiple factors
    score = 0

    # 1. Clickbait indicators
    cb_score = clickbait_score(text)
    if cb_score > 0.5:
        score += 2
    elif cb_score > 0.3:
        score += 1

    # 2. Misinformation keywords
    keyword_count = sum(1 for kw in MISINFO_KEYWORDS if kw.lower() in text)
    score += keyword_count

    # 3. High-risk subreddits
    if subreddit in ['conspiracy', 'the_donald']:
        score += 2
    elif SUBREDDIT_RISK.get(subreddit, 0.4) > 0.6:
        score += 1

    # 4. Specific patterns
    if any(word in text for word in ['proof', 'exposed', 'they dont want', 'wake up', 'leaked']):
        score += 1

    if '!!!' in text or text.count('!') >= 3:
        score += 1

    # Label as misinformation if score >= 3
    return 1 if score >= 3 else 0


def scrape_training_data():
    """
    Scrape real Reddit posts from various hashtags
    """
    print("=" * 60)
    print("SCRAPING REAL REDDIT DATA FOR TRAINING")
    print("=" * 60)

    # Hashtags likely to have misinformation
    misinfo_hashtags = ["#conspiracy", "#leaked", "#exposed", "#vaccine"]

    # Hashtags likely to have normal posts
    normal_hashtags = ["#gaming", "#technology", "#help", "#discussion"]

    all_records = []
    all_labels = []

    print("\nScraping misinformation-prone hashtags...")
    for hashtag in misinfo_hashtags:
        print(f"  Searching {hashtag}...")
        try:
            posts = search_reddit_by_hashtag(hashtag, num_results=15)
            for post in posts:
                label = auto_label_post(post)
                all_records.append(post)
                all_labels.append(label)
            print(f"    ‚úì Found {len(posts)} posts")
        except Exception as e:
            print(f"    ‚úó Error: {e}")

    print("\nScraping normal hashtags...")
    for hashtag in normal_hashtags:
        print(f"  Searching {hashtag}...")
        try:
            posts = search_reddit_by_hashtag(hashtag, num_results=15)
            for post in posts:
                label = auto_label_post(post)
                all_records.append(post)
                all_labels.append(label)
            print(f"    ‚úì Found {len(posts)} posts")
        except Exception as e:
            print(f"    ‚úó Error: {e}")

    return all_records, np.array(all_labels)


def main():
    """
    Main training pipeline: scrape, label, train
    """
    print("\n" + "=" * 60)
    print("AUTOMATED MODEL TRAINING")
    print("=" * 60)

    # Step 1: Scrape real data
    print("\nStep 1: Scraping real Reddit posts from Google...")
    try:
        records, labels = scrape_training_data()

        # If scraping returned empty results, use fallback
        if len(records) == 0:
            raise ValueError("No posts scraped - using fallback data")

    except Exception as e:
        print(f"\n‚ö†Ô∏è  Scraping issue: {e}")
        print("üìã Using fallback synthetic data for training...")

        # Fallback to synthetic data
        POSITIVE = ["New study CONFIRMS coffee cures cancer", "Official leak: Tesla TOMORROW!!!",
                   "They don't want you to know this", "Proof that vaccine is fake", "Company exposed as scam"]
        NEGATIVE = ["How do I choose a car?", "Best graphics settings", "Fan art thread",
                   "Help diagnosing oil leak", "Favorite game soundtrack"]

        records = []
        labels = []
        for t in POSITIVE:
            records.append({"title": t, "snippet": t, "url": "", "subreddit": "news", "rank": 1})
            labels.append(1)
        for t in NEGATIVE:
            records.append({"title": t, "snippet": t, "url": "", "subreddit": "general", "rank": 1})
            labels.append(0)
        labels = np.array(labels)

    # Show statistics
    misinfo_count = sum(labels)
    normal_count = len(labels) - misinfo_count
    print(f"\n‚úì Collected {len(records)} posts")
    print(f"  - Auto-labeled as misinformation: {misinfo_count}")
    print(f"  - Auto-labeled as normal: {normal_count}")

    # Step 2: Build features
    print("\nStep 2: Extracting features...")
    X_text, X_eng, vectorizer = build_feature_matrix(records, vectorizer=None)
    X = hstack([X_text, X_eng])
    print(f"‚úì Feature matrix shape: {X.shape}")

    # Step 3: Split data into train and test sets
    print("\nStep 3: Splitting data into train/test sets...")
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, labels, test_size=0.2, random_state=42, stratify=labels
    )
    print(f"‚úì Training samples: {X_train.shape[0]}")
    print(f"‚úì Test samples: {X_test.shape[0]}")

    # Step 4: Train model
    print("\nStep 4: Training Logistic Regression model...")
    clf = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
    clf.fit(X_train, y_train)

    # Show accuracy on both train and test
    train_acc = clf.score(X_train, y_train)
    test_acc = clf.score(X_test, y_test)
    print(f"‚úì Training accuracy: {train_acc * 100:.1f}%")
    print(f"‚úì Test accuracy: {test_acc * 100:.1f}%")

    if train_acc > test_acc + 0.15:
        print(f"‚ö†Ô∏è  Warning: Large gap between train and test accuracy suggests overfitting!")

    # Step 5: Save model (train on full dataset for production)
    print("\nStep 5: Re-training on full dataset and saving model...")
    clf.fit(X, labels)  # Train on all data for production use
    joblib.dump(clf, "misinfo_logreg_model.joblib")
    joblib.dump(vectorizer.tfidf, "tfidf_vectorizer.joblib")
    print("‚úì Model saved: misinfo_logreg_model.joblib")
    print("‚úì Vectorizer saved: tfidf_vectorizer.joblib")

    print("\n" + "=" * 60)
    print("SUCCESS! Model trained with real scraped data")
    print("=" * 60)
    print("\nYou can now run: python3 app.py")
    print("The web server will use this trained model.")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    main()
